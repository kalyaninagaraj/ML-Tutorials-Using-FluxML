{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics, DelimitedFiles\n",
    "using Flux: params, gradient\n",
    "using Flux.Optimise: update!, Descent\n",
    "using Flux: mse, @epochs\n",
    "using Statistics: mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14×505 adjoint(::Matrix{Float64}) with eltype Float64:\n",
       "   0.02731    0.02729    0.03237  …    0.06076    0.10959    0.04741\n",
       "   0.0        0.0        0.0           0.0        0.0        0.0\n",
       "   7.07       7.07       2.18         11.93      11.93      11.93\n",
       "   0.0        0.0        0.0           0.0        0.0        0.0\n",
       "   0.469      0.469      0.458         0.573      0.573      0.573\n",
       "   6.421      7.185      6.998    …    6.976      6.794      6.03\n",
       "  78.9       61.1       45.8          91.0       89.3       80.8\n",
       "   4.9671     4.9671     6.0622        2.1675     2.3889     2.505\n",
       "   2.0        2.0        3.0           1.0        1.0        1.0\n",
       " 242.0      242.0      222.0         273.0      273.0      273.0\n",
       "  17.8       17.8       18.7      …   21.0       21.0       21.0\n",
       " 396.9      392.83     394.63        396.9      393.45     396.9\n",
       "   9.14       4.03       2.94          5.64       6.48       7.88\n",
       "  21.6       34.7       33.4          23.9       22.0       11.9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the housing data\n",
    "cd(@__DIR__)\n",
    "isfile(\"HousingData/housing.data\") ||\n",
    "  download(\"https://raw.githubusercontent.com/MikeInnes/notebooks/master/housing.data\",\n",
    "           \"HousingData/housing.data\")\n",
    "\n",
    "rawdata = readdlm(\"HousingData/housing.data\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×455 Matrix{Float64}:\n",
       " 20.5  25.0  23.4  18.9  35.4  24.7  …  16.8  22.4  20.6  23.9  22.0  11.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The last feature is our target: the price of the house.\n",
    "split_ratio = 0.1 # For the train test split\n",
    "\n",
    "x = rawdata[1:13,:]\n",
    "y = rawdata[14:14,:]\n",
    "\n",
    "# Normalise the data\n",
    "x = (x .- mean(x, dims = 2)) ./ std(x, dims = 2)\n",
    "\n",
    "# Split into train and test sets\n",
    "split_index = floor(Int,size(x,2)*split_ratio)\n",
    "x_train = x[:,1:split_index]\n",
    "y_train = y[:,1:split_index]\n",
    "x_test = x[:,split_index+1:size(x,2)]\n",
    "y_test = y[:,split_index+1:size(x,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## FIRST WAY: Manually set up a least squares model\n",
    "W = params(rand(1,13)/10)\n",
    "b = params([0.1])\n",
    "\n",
    "predict(x) = W*x .+ b\n",
    "loss(x, y) = mse(predict(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: in(::Zygote.Params, ::Zygote.IdSet{Any}) is ambiguous. Candidates:\n  in(x, s::Zygote.IdSet) in Zygote at /Users/kalyani/.julia/packages/Zygote/RxTZu/src/tools/idset.jl:12\n  in(x::Zygote.Params, args...; kwargs...) in Zygote at /Users/kalyani/.julia/packages/MacroTools/gME9C/src/examples/forward.jl:17\nPossible fix, define\n  in(::\u001b[0mZygote.Params, ::\u001b[0mZygote.IdSet)",
     "output_type": "error",
     "traceback": [
      "MethodError: in(::Zygote.Params, ::Zygote.IdSet{Any}) is ambiguous. Candidates:\n  in(x, s::Zygote.IdSet) in Zygote at /Users/kalyani/.julia/packages/Zygote/RxTZu/src/tools/idset.jl:12\n  in(x::Zygote.Params, args...; kwargs...) in Zygote at /Users/kalyani/.julia/packages/MacroTools/gME9C/src/examples/forward.jl:17\nPossible fix, define\n  in(::\u001b[0mZygote.Params, ::\u001b[0mZygote.IdSet)",
      "",
      "Stacktrace:",
      " [1] params!(p::Zygote.Params, x::Zygote.Params, seen::Zygote.IdSet{Any})",
      "   @ Flux ~/.julia/packages/Flux/qp1gc/src/functor.jl:44",
      " [2] params! (repeats 2 times)",
      "   @ ~/.julia/packages/Flux/qp1gc/src/functor.jl:47 [inlined]",
      " [3] params!",
      "   @ ~/.julia/packages/Flux/qp1gc/src/functor.jl:44 [inlined]",
      " [4] params(m::Vector{Zygote.Params})",
      "   @ Flux ~/.julia/packages/Flux/qp1gc/src/functor.jl:53",
      " [5] top-level scope",
      "   @ In[9]:4",
      " [6] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [7] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "## TRAIN YOUR MODEL:\n",
    "## ONE WAY: Code your own gradient descent\n",
    "η = 0.1\n",
    "θ = params([W, b])\n",
    "\n",
    "for i = 1:100\n",
    "  g = gradient(() -> loss(x_train, y_train), θ)\n",
    "  for x in θ\n",
    "    update!(x, -g[x]*η)\n",
    "  end\n",
    "  @show loss(x_train, y_train), loss(x_test,y_test)\n",
    "end\n",
    "\n",
    "# Report MSE on the test set\n",
    "test_err = loss(x_test,y_test)\n",
    "println(test_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SECOND WAY: Train the model using Flux's inbuilt gradient descent optimizer\n",
    "η = 0.1\n",
    "θ = Params([W, b])\n",
    "\n",
    "# Gradient descent optimiser with learning rate 0.5.\n",
    "optimiser = Descent(η)\n",
    "\n",
    "# Create iterator to train model over 100 epochs.\n",
    "data_iterator = Iterators.repeated((x_train, y_train), 100)\n",
    "\n",
    "# Call back\n",
    "evalcb() = @show(loss(x_train, y_train))\n",
    "\n",
    "# Report MSE on the test set\n",
    "test_err = loss(x_test,y_test)\n",
    "println(test_err)\n",
    "\n",
    "println(\"Starting training.\")\n",
    "Flux.train!(loss, θ, data_iterator, optimiser, cb = evalcb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIRD WAY: Use epochs to loop over data set multiple times\n",
    "η = 0.3\n",
    "θ = Params([W, b])\n",
    "\n",
    "# Gradient descent optimiser with learning rate 0.5.\n",
    "optimiser = Descent(η)\n",
    "\n",
    "# Create iterator to train model over 100 epochs.\n",
    "data_iterator = Iterators.repeated((x_train, y_train), 100)\n",
    "\n",
    "# Call back\n",
    "evalcb() = @show(loss(x_train, y_train))\n",
    "\n",
    "println(\"Starting training\")\n",
    "@epochs 1 Flux.train!(loss, θ, data_iterator, optimiser, cb = evalcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report MSE on the test set\n",
    "test_err = loss(x_test,y_test)\n",
    "println(\"Test error = $(test_err)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
